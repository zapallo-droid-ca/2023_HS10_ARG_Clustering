{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5ad372-ec94-40f6-9a72-b17ffdff4dbb",
   "metadata": {},
   "source": [
    "## **EXPLORATORY DATA ANALYSIS** \n",
    "**EXP 03. TIME SERIES DECOMPOSITION CLUSTERING** <br>\n",
    "*ARGENTINIAN MAIZE INTERNATIONAL TRADE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f387e7f-90f7-4e93-8d71-01930520cd9f",
   "metadata": {},
   "source": [
    "**DISCLAIMER:** <br>\n",
    "*The data used for analyzing the market was obtained from the United Nations Stats and other open sources and the present project only has non-profit meanings, has been developed just to show analytic skills as part of a personal project portfolio so, the information and insights in the present document can't be used with commercial purposes keeping each data source with their original licences.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f99134-0a08-4cd5-9503-8e0f7da4cb33",
   "metadata": {},
   "source": [
    "**INTRODUCTION:** <br>\n",
    "It is usual to find time series problems when we are working in business areas, those can be series about sales, stock levels, product demands or even data about machinery failures and maintenance. This kind of problem is used to be defined with high dimensional characteristics due to the granularity level that business requires and the main way to work with used to be aggregating the dataset with particular business metrics in a determined time period which misses part of the information contained in the dataset regarding the characteristics of the time series itself and about the relationships between the values among the timestamps, one clear example could be that you can lose information about the seasonality behaviour and the patterns among the analyzed dimensions.\n",
    "\n",
    "Clustering algorithms as part of an Exploratory Data Analysis are powerful tools to identify hidden patterns in data and when working with time series we can take two approaches, the most frequent in my experience (and not necessarily the best) is the above-mentioned, aggregating the dataset but, we can bring to the analysis the time series characteristics using the proper techniques.\n",
    "In this project, we are going to explore three clustering alternatives to work with time series using a dataset with the Argentinian Maize Exports by partner country:\n",
    "* Aggregating the dataset and working with basic business metrics\n",
    "* Working with the Dynamic Time Warping over the target variable time series\n",
    "* Decomposing the time series into its three components *(current notebook)*\n",
    "\n",
    "\n",
    "**OBJECTIVE:** <br>\n",
    "Identify hidden patterns in data using the Time series components and an Clustering technique over the trade partners\n",
    "\n",
    "\n",
    "**PIPELINE:** <br>\n",
    "* Dataset transformation into a classification problem.\n",
    "* Dataset preprocessing.\n",
    "* Optimal number of clusters definitions.\n",
    "* Partners clustering experiments and clusters definitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9b199-c3a1-4c6b-acf4-6fbf0c88372a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 00.00. RESOURCES AND WORK ENVIRONMENT SETTING\n",
    "\n",
    "To develop this projects, we need to prepare the environment and that involves loading the required libraries and to defined the work directory where:\n",
    "\n",
    "**Data Handling Libraries:**\n",
    "* **pandas:** working with dataframes\n",
    "* **numpy:** handling numbers\n",
    "\n",
    "**Graphic Libraries:**\n",
    "* **matplotlib:** creating graphic objects\n",
    "* **seaborn:** creating graphic objects\n",
    "\n",
    "**Clustering Resources:**\n",
    "* **sklearn:** Machine Learning library, used to data preprocessing and also for clustering creation and definition\n",
    "* **fastdtw:** Dynamic Time Warping implementation\n",
    "* **seasonal_decompose**: Time Series Decomposition\n",
    "\n",
    "**General Config Libraries:**\n",
    "* **os:** workdirectory path definition\n",
    "* **warnings:** ignore warnings messages from libraries\n",
    "\n",
    "#### 00.01. LIBRARIES AND WD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065505a5-c732-4607-adbc-d410167519f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- ENVIRONMENT SETTINGS\n",
    "import pandas as pd ; from pandas.tseries.offsets import MonthEnd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425f659c-d3a3-471d-a22c-c721ebbd2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- GENERAL SETTINGS\n",
    "csvAttr_imp = {'sep': ';' , 'encoding': 'UTF-8'} #csv settings - export\n",
    "\n",
    "cereal = 'Maize'\n",
    "maxClusterNumbers = 6\n",
    "randomStateValue = 2023\n",
    "flowType = 'export'\n",
    "\n",
    "flowColumn = 't_exp' if flowType.lower() == 'export' else 't_imp'\n",
    "flowColumnInv = 't_imp' if flowType.lower() == 'export' else 't_exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb14558-e8a1-4df0-9ede-b17588c48bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- WORK DIRECTORY\n",
    "wd = 'C:/Users/jrab9/OneDrive/08.Github/2023.HS10-ARG.Clustering/01.Data/'\n",
    "\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52dba58-fa3b-4f7c-83ee-7dfd0fcae65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- AD-HOC FUNCTIONS\n",
    "def tsDecomposition(df_base, cereal, flowType): #To transform the main serie in 3 new dataframes with each component    \n",
    "    #flowColumn = 't_exp' if flowType.lower() == 'export' else 't_imp'\n",
    "    \n",
    "    workPaper = df_base[df_base['desc_l2'] == cereal].reset_index(drop = True).copy()[['date','partnerCodeISO',flowColumn]]  \n",
    "    \n",
    "    partnerCodes = workPaper.partnerCodeISO.unique()\n",
    "    \n",
    "    df_trend = pd.DataFrame()\n",
    "    df_seasonal = pd.DataFrame()\n",
    "    df_residual = pd.DataFrame() \n",
    "    \n",
    "    for partner in partnerCodes:\n",
    "        df_temp = workPaper[workPaper['partnerCodeISO'] == partner]        \n",
    "        \n",
    "        #-SCALING\n",
    "        scaler = StandardScaler()    \n",
    "        df_temp[flowColumn] = scaler.fit_transform(df_temp[flowColumn].values.reshape(-1,1))\n",
    "\n",
    "        ##INDEX\n",
    "        df_temp.set_index(pd.to_datetime(df_temp['date'], format = '%Y-%m-%d'), inplace = True)\n",
    "        df_temp = df_temp.asfreq('m')\n",
    "        \n",
    "        #-TIME SERIE DECOMPOSITION\n",
    "        sdModel = 'additive'\n",
    "        sdDF = seasonal_decompose(df_temp[flowColumn], model = sdModel)\n",
    "\n",
    "        clusteredData = pd.DataFrame({'trend': sdDF.trend,\n",
    "                                      'seasonal': sdDF.seasonal,\n",
    "                                      'residual': sdDF.resid\n",
    "                                     })\n",
    "\n",
    "        df_temp_trend = clusteredData.trend.T\n",
    "        df_temp_trend['partnerCodeISO'] = partner\n",
    "        df_temp_trend['flowType'] = flowType\n",
    "        df_temp_trend['component'] = 'trend'\n",
    "\n",
    "        df_temp_seasonal = clusteredData.seasonal.T\n",
    "        df_temp_seasonal['partnerCodeISO'] = partner\n",
    "        df_temp_seasonal['flowType'] = flowType\n",
    "        df_temp_seasonal['component'] = 'seasonal'\n",
    "\n",
    "        df_temp_resid = clusteredData.residual.T\n",
    "        df_temp_resid['partnerCodeISO'] = partner\n",
    "        df_temp_resid['flowType'] = flowType\n",
    "        df_temp_resid['component'] = 'residual'    \n",
    "\n",
    "        df_temp_trend['cereal'] = cereal  \n",
    "        df_temp_seasonal['cereal'] = cereal \n",
    "        df_temp_resid['cereal'] = cereal  \n",
    "\n",
    "        df_trend = df_trend.append(df_temp_trend)\n",
    "        df_seasonal = df_seasonal.append(df_temp_seasonal)\n",
    "        df_residual = df_residual.append(df_temp_resid)     \n",
    "\n",
    "        del(df_temp_trend,df_temp_seasonal,df_temp_resid,sdDF,clusteredData, scaler)   \n",
    "        print(f'{flowType} of {cereal} done for partner {partner}', end = '\\r')                   \n",
    "          \n",
    "    df_trend.set_index(df_trend.partnerCodeISO, inplace = True) ; df_seasonal.set_index(df_seasonal.partnerCodeISO, inplace = True) ; df_residual.set_index(df_residual.partnerCodeISO, inplace = True)\n",
    "    df_trend.columns.name = None ; df_seasonal.columns.name = None ; df_residual.columns.name = None  \n",
    "    \n",
    "    df_timeSeriesDecomp = df_trend.drop(columns = ['cereal']).melt(id_vars = ['partnerCodeISO','flowType','component'], var_name = 'date')\n",
    "    df_timeSeriesDecomp = df_timeSeriesDecomp.append(df_seasonal.drop(columns = ['cereal']).melt(id_vars = ['partnerCodeISO','flowType','component'], var_name = 'date'))\n",
    "    df_timeSeriesDecomp = df_timeSeriesDecomp.append(df_residual.drop(columns = ['cereal']).melt(id_vars = ['partnerCodeISO','flowType','component'], var_name = 'date'))\n",
    "    df_timeSeriesDecomp\n",
    "\n",
    "    print(f'process ended, df_trend: {df_trend.shape}, df_seasonal: {df_seasonal.shape}, df_residual: {df_residual.shape}, df_timeSeriesDecomp: {df_timeSeriesDecomp.shape}')\n",
    "\n",
    "    return df_trend, df_seasonal, df_residual, df_timeSeriesDecomp\n",
    "\n",
    "def grxClusters(df, target, flowType, clusterIndex = 0, scaled = False, MinMax = True):     \n",
    "    colPrefix = f'{optimalNumberOfClusters[clusterIndex]}_clusters{target.title()}'\n",
    "    #flowColumn = 't_exp' if flowType.lower() == 'export' else 't_imp'\n",
    "    \n",
    "    numberOfClustersToPlot = optimalNumberOfClusters[clusterIndex]\n",
    "    numberOfCols = 4 if numberOfClustersToPlot >= 4 else int(numberOfClustersToPlot)\n",
    "    numberOfRows = int(np.ceil(numberOfClustersToPlot / numberOfCols))\n",
    "\n",
    "    figShapeHeight = numberOfRows * 4\n",
    "    figShapeWidth = numberOfCols * 6\n",
    "    \n",
    "    df = df.reset_index(drop = False).fillna(0)\n",
    "    \n",
    "    if scaled: #Scaling each country series, not all the dataset\n",
    "        df_scaled = pd.DataFrame()\n",
    "        for partner in partnerCodes:\n",
    "            df_partner_temp = df[df['partnerCodeISO'] == partner].reset_index(drop = True)\n",
    "            \n",
    "            #-SEPARING NUMERIC AND CATEGORIC VARS\n",
    "            numCols = [flowColumn]\n",
    "            catCols = df_partner_temp.columns[df_partner_temp.columns != flowColumn]\n",
    "\n",
    "            #-SEPARATING DATA\n",
    "            df_tempNum = df_partner_temp[numCols] ; df_tempCat = df_partner_temp[catCols]\n",
    "            #-SCALING\n",
    "            if MinMax == True:\n",
    "                scaler = MinMaxScaler()  \n",
    "            else:\n",
    "                scaler = StandardScaler()        \n",
    "            scaledData = scaler.fit_transform(df_tempNum)\n",
    "\n",
    "            #-SCALED DATA\n",
    "            df_partner_temp_scaled = pd.DataFrame(scaledData, columns=numCols)\n",
    "            df_partner_temp = pd.concat([df_partner_temp_scaled, df_tempCat], axis=1) \n",
    "            \n",
    "            df_scaled = df_scaled.append(df_partner_temp)\n",
    "            \n",
    "        df = df_scaled.copy().fillna(0) ; del(df_scaled)         \n",
    "        \n",
    "    if scaled:\n",
    "        if MinMax:\n",
    "            titleExtension = 'each series (individually) scaled by MinMax Normalization'\n",
    "        else:\n",
    "            titleExtension = 'each series (individually) scaled by Z-Score'\n",
    "    else:\n",
    "        titleExtension = 'units of cereal in thousand tonnes'\n",
    "        \n",
    "    #-MEANS BY CLUSTER\n",
    "    df_clustersMeans = pd.DataFrame({'date':df['date']}).drop_duplicates()  \n",
    "       \n",
    "    for clusterId in range(numberOfClustersToPlot):\n",
    "        clusterName = f'c{clusterId}'\n",
    "        df_cluster = df[df[f'{colPrefix}'] == clusterId].groupby('date').agg(var=(flowColumn, 'mean')).reset_index().rename(columns={'var': clusterName})\n",
    "        if scaled == False:         \n",
    "            df_cluster[clusterName] = df_cluster[clusterName] / 1000\n",
    "        df_clustersMeans = df_clustersMeans.merge(df_cluster[['date', clusterName]], on='date', how='left')\n",
    "        \n",
    "    df_clustersMeans = df_clustersMeans.sort_values('date', ascending = True).set_index('date').dropna()\n",
    "    \n",
    "    #-FIGURE CONFIG\n",
    "    fig, axes = plt.subplots(nrows=numberOfRows, ncols=numberOfCols, sharey=False, sharex=False, figsize=(figShapeWidth, figShapeHeight))\n",
    "    fig.suptitle(f'K-means: {flowType.capitalize()}s of {cereal} Time Series Clusterized by {target} Component in {numberOfClustersToPlot} Clusters\\n{titleExtension}')\n",
    "\n",
    "    #-ITERATING OVER CLUSTERS\n",
    "    for clusterID, ax in zip(range(numberOfClustersToPlot), axes.flatten()):\n",
    "        clusterName = f'c{clusterID}'        \n",
    "       \n",
    "        ##-CLUSTER DATA\n",
    "        for partner in partnerCodes:\n",
    "            df_temp = df[(df[f'{colPrefix}'] == clusterID) & (df['partnerCodeISO'] == partner)][['date', flowColumn]].sort_values('date', ascending = True).set_index('date').dropna()\n",
    "            if scaled == False:      \n",
    "                df_temp[flowColumn] = df_temp[flowColumn] / 1000\n",
    "            ax.plot(df_temp[flowColumn])\n",
    "        \n",
    "        #-CLUSTER MEANS ----------\n",
    "        ax.plot(df_clustersMeans[clusterName], linestyle='--', color='black')\n",
    "        ax.set_title(f'Cluster {clusterName}', y = -0.2)\n",
    "            \n",
    "    return fig\n",
    "\n",
    "def grxClusters_stdDecomp(df, target, flowType, clusterIndex = 0):  \n",
    "    colPrefix = f'{optimalNumberOfClusters[clusterIndex]}_clusters{target.title()}'\n",
    "    \n",
    "    df = df[df['flowType'] == flowType].reset_index(drop = False)\n",
    "    \n",
    "    numberOfClustersToPlot = optimalNumberOfClusters[clusterIndex]\n",
    "    numberOfCols = 4 if numberOfClustersToPlot >= 4 else int(numberOfClustersToPlot)\n",
    "    numberOfRows = int(np.ceil(numberOfClustersToPlot / numberOfCols))\n",
    "\n",
    "    figShapeHeight = numberOfRows * 4\n",
    "    figShapeWidth = numberOfCols * 6\n",
    "    \n",
    "    #-MEANS BY CLUSTER\n",
    "    #**** #####---DEPRECATED: The following will calculate the average value among series,we want to show the centroid value of each cluster\n",
    "    #**** #df_clustersMeans = pd.DataFrame({'date':df['date']}).drop_duplicates()  \n",
    "       \n",
    "    #**** #for clusterId in range(numberOfClustersToPlot):\n",
    "    #**** #    clusterName = f'c{clusterId}'\n",
    "    #**** #    df_cluster = df[df[f'{colPrefix}'] == clusterId].groupby('date').agg(var=('value', 'mean')).reset_index().rename(columns={'var': clusterName})\n",
    "    #**** #    df_clustersMeans = df_clustersMeans.merge(df_cluster[['date', clusterName]], on='date', how='left')\n",
    "        \n",
    "    #**** #df_clustersMeans = df_clustersMeans.sort_values('date', ascending = True).set_index('date').dropna()\n",
    "    \n",
    "    #Here we are taking the centroid value\n",
    "    df_clustersMeans = kmeansCenters[kmeansCenters['exp'] == newColumns[clusterIndex]].reset_index(drop = True).drop(columns = 'partners').melt(id_vars = ['cluster','exp']).sort_values('variable').rename(columns = {'variable':'date','value':flowColumn})\n",
    "    \n",
    "    #-FIGURE CONFIG\n",
    "    fig, axes = plt.subplots(nrows=numberOfRows, ncols=numberOfCols, sharey=False, sharex=False, figsize=(figShapeWidth, figShapeHeight))\n",
    "    fig.suptitle(f'K-means: {flowType.capitalize()}s of {cereal} Time Series Clusterized by {target} Component in {numberOfClustersToPlot} Clusters\\nSeries Scaled by Z-Score')\n",
    "\n",
    "    #-ITERATING OVER CLUSTERS\n",
    "    for clusterID, ax in zip(range(numberOfClustersToPlot), axes.flatten()):\n",
    "        clusterName = f'c{clusterID}'        \n",
    "       \n",
    "        ##-CLUSTER DATA\n",
    "        for partner in partnerCodes:\n",
    "            df_temp = df[(df[f'{colPrefix}'] == clusterID) & (df['partnerCodeISO'] == partner)][['date', 'value']].sort_values('date', ascending = True).set_index('date').dropna()\n",
    "            ax.plot(df_temp['value'])\n",
    "        \n",
    "        #-CLUSTER MEANS \n",
    "        #**** #ax.plot(df_clustersMeans[clusterName], linestyle='--', color='black')\n",
    "        ax.plot(df_clustersMeans[df_clustersMeans['cluster'] == clusterID][['date',flowColumn]].reset_index(drop = True).sort_values('date',ascending = True).set_index('date').fillna(0), linestyle='--', color='black')\n",
    "        ax.set_title(f'{clusterName}',y = -0.2)\n",
    "            \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac86abd-c063-41c7-b4d4-3985200839d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 00.02 DATA\n",
    "\n",
    "The dataset has been cleaned and curated for this problem in the file etl.py and it contains data for all partners in every single timestamp, for those months with no transactions the variables numeric will take 0 as value and it doesn’t contain any Null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ab2237-252a-4cc7-86c5-fe189a390313",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- DATA\n",
    "colsToKeep = ['calendarCode','date',flowColumn,'reporterCodeISO','reporterDesc','partnerCodeISO','partnerDesc','partnerRegionDesc','desc_l2']\n",
    "\n",
    "df_base = pd.read_csv(wd + '/dfM.csv.gz', sep = csvAttr_imp['sep'], encoding = csvAttr_imp['encoding'])\n",
    "df_base['date'] = pd.to_datetime(df_base['date'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c53ac15-aa79-44eb-a530-3ff192556688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame shape: (74752, 9)\n",
      "Time series range from 2016-12-31 to 2022-12-31 (73 meses)\n"
     ]
    }
   ],
   "source": [
    "##-- DATA\n",
    "colsToKeep = ['calendarCode','date',flowColumn,'reporterCodeISO','reporterDesc','partnerCodeISO','partnerDesc','partnerRegionDesc','desc_l2']\n",
    "\n",
    "df_base = pd.read_csv(wd + '/dfM.csv.gz', sep = csvAttr_imp['sep'], encoding = csvAttr_imp['encoding'])\n",
    "df_base['date'] = pd.to_datetime(df_base['date'], format = '%Y-%m-%d')\n",
    "\n",
    "df_base_copy = df_base[np.append(colsToKeep,flowColumnInv)].copy()\n",
    "\n",
    "df_base = df_base[colsToKeep]\n",
    "\n",
    "##-- DATASET FACTS\n",
    "dfShape = df_base.shape\n",
    "dfMinDate = df_base.date.min().date()\n",
    "dfMaxDate = df_base.date.max().date()\n",
    "dfMonths = len(pd.date_range(dfMinDate,dfMaxDate, freq = 'm'))\n",
    "\n",
    "print(f'Data Frame shape: {dfShape}')\n",
    "print(f'Time series range from {dfMinDate} to {dfMaxDate} ({dfMonths} meses)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb09e0-1e3b-4653-b29a-9bc597fe2826",
   "metadata": {},
   "source": [
    "The dataset contains 74752 rows (observations), each representing one cereal and partner transaction in one month between the above-printed range, and it can be statistically described as follows, at this point this data wont be useful due that we have many cereals and partners time series appended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b973c4-133b-4106-9753-cb4320075dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7.475200e+04\n",
       "mean     4.252870e+03\n",
       "std      4.530350e+04\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      0.000000e+00\n",
       "75%      0.000000e+00\n",
       "max      2.326720e+06\n",
       "Name: t_exp, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-- NUMERIC COLUMNS RELATED TO EXPORTS\n",
    "df_base[flowColumn].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f6b16c-1864-4e14-9e9f-3f646c06bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>reporterCodeISO</th>\n",
       "      <th>reporterDesc</th>\n",
       "      <th>partnerCodeISO</th>\n",
       "      <th>partnerDesc</th>\n",
       "      <th>partnerRegionDesc</th>\n",
       "      <th>desc_l2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>74752</td>\n",
       "      <td>74752</td>\n",
       "      <td>74752</td>\n",
       "      <td>74752</td>\n",
       "      <td>74752</td>\n",
       "      <td>74752</td>\n",
       "      <td>74752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2016-12-31 00:00:00</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>BDI</td>\n",
       "      <td>Burundi</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Wheat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1024</td>\n",
       "      <td>74752</td>\n",
       "      <td>74752</td>\n",
       "      <td>584</td>\n",
       "      <td>584</td>\n",
       "      <td>21608</td>\n",
       "      <td>9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>2016-12-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>2022-12-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date reporterCodeISO reporterDesc partnerCodeISO  \\\n",
       "count                 74752           74752        74752          74752   \n",
       "unique                   73               1            1            128   \n",
       "top     2016-12-31 00:00:00             ARG    Argentina            BDI   \n",
       "freq                   1024           74752        74752            584   \n",
       "first   2016-12-31 00:00:00             NaN          NaN            NaN   \n",
       "last    2022-12-31 00:00:00             NaN          NaN            NaN   \n",
       "\n",
       "       partnerDesc partnerRegionDesc desc_l2  \n",
       "count        74752             74752   74752  \n",
       "unique         128                 6       8  \n",
       "top        Burundi            Africa   Wheat  \n",
       "freq           584             21608    9344  \n",
       "first          NaN               NaN     NaN  \n",
       "last           NaN               NaN     NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-- CATEGORICAL & DT VARIABLES\n",
    "df_base[df_base.columns[df_base.columns != flowColumn]].drop(columns = 'calendarCode').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40703394-f8f7-4eb7-9e63-cc8895466a31",
   "metadata": {},
   "source": [
    "### 01.00. TRANSFORMATIONS AND PREPROCESSING\n",
    "\n",
    "In the current state, the dataset is not ready for clustering, now we have many time series appended in rows, and we need to prepare the dataset to be used in a classification problem, for this the timestamps will be pivoted, to Each time series will be presented in a single row for each partner.\n",
    "\n",
    "The main steps behind the transformation process are:\n",
    "1. To pivot the dataset through its timestamps, getting one partner trade time series in each row.\n",
    "2. To escalate the dataset using Z-score technique\n",
    "3. Extracting each time series component trend, seasonal and residual (or random) components\n",
    "\n",
    "*Observations:* Only the countries with transactions during the last year (2022 in the dataset) will be considered to the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b9a6db-4f78-4500-a6aa-238c53a8b1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4088, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##-FILTERING DF: ONLY CEREAL OF INTEREST\n",
    "df_base = df_base[df_base['desc_l2'] == cereal].reset_index(drop = True).copy()\n",
    "\n",
    "##-FILTERING DF: ONLY PARTNERS WITH COMMERCE IN THE LAST YEAR\n",
    "flowColumn = 't_exp' if flowType.lower() == 'export' else 't_imp'\n",
    "\n",
    "partnersOfInterest = df_base[['partnerCodeISO',flowColumn,'date']][df_base['date'].dt.year == df_base['date'].dt.year.max()].groupby(['partnerCodeISO']).agg(val = (flowColumn,'sum')).reset_index()\n",
    "partnersOfInterest = partnersOfInterest[partnersOfInterest['val'] != 0].reset_index(drop = True)['partnerCodeISO']\n",
    "partnersOfInterest\n",
    "\n",
    "df_base = df_base[df_base['partnerCodeISO'].isin(partnersOfInterest)].reset_index(drop = True).copy()\n",
    "df_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3931016-28b9-43e7-b5c9-5dd12eddca22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Maize'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.desc_l2.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272d9a1-6d0a-4027-8876-ebe241406059",
   "metadata": {},
   "source": [
    "**Preprocessing and Transformation:**\n",
    "\n",
    "The transformation and preprocessing have been consolidated into the function “tsDecomposition“ where each partner series have been standardized individually.\n",
    "\n",
    "The result of the function is a dataset for each component and a consolidated dataset to plot in one of the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e95e90-6bdf-4718-a02f-6c63e392c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process ended, df_trend: (56, 77), df_seasonal: (56, 77), df_residual: (56, 77), df_timeSeriesDecomp: (12264, 5)\n"
     ]
    }
   ],
   "source": [
    "df_trend, df_seasonal, df_residual, df_timeSeriesDecomp = tsDecomposition(df_base = df_base, cereal = cereal, flowType = flowType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6703143-ff7d-464a-8f58-b74244babd14",
   "metadata": {},
   "source": [
    "### 02.00. CLUSTERING BY TREND COMPONENT\n",
    "#### 02.01. OPTIMAL NUMBER OF CLUSTERS\n",
    "\n",
    "For this exercise, the k-means algorithm has been selected due to its interpretability (centroid means) and to the nature of the problem (unsupervised). Before creating the clusters, we need to answer a particular question: how many clusters do we need? this question could has multiple answers, and it depends on two main issues:\n",
    "* Is there any need for the business? for example, this can be a determined number of clusters for a particular campaign. \n",
    "* If we don’t have any constraints or requirements, we can work supported with clustering quality metrics, visual analysis and considering business knowledge (this is a key piece to delivering better quality solutions)\n",
    "\n",
    "To determine the optimal number of clustering we are going to use three metrics:\n",
    "1. *Silhouette coefficient:* This coefficient considers the mean intra-cluster distance (cohesion) and the mean nearest-cluster distance (separation) for each sample and takes values between -1 and 1 where the highest values indicate better matches to its cluster.\n",
    "2. *Calinski Harabasz Score:* This score is calculated considering the ratio of between-cluster dispersion and of within-cluster dispersion. The highest values indicate a better definition of the cluster.\n",
    "3. *Davies Bouldin Index:* This index measures the average similarity between clusters comparing the distances between clusters with the size of the cluster. The lowest value possible is Zero and the closer the value to zero means a better cluster partition.\n",
    "\n",
    "For further information, https://scikit-learn.org/stable/modules/clustering.html presents a good summary of clustering concepts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df9f7c-da44-4399-9c09-badab4946ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- OPTIMAL CLUSTER NUMBERS\n",
    "target = 'trend'\n",
    "\n",
    "if target == 'trend':\n",
    "    df_temp = df_trend.copy()\n",
    "elif target == 'seasonal':\n",
    "    df_temp = df_seasonal.copy()\n",
    "elif target == 'residual':\n",
    "    df_temp = df_residual.copy()\n",
    "    \n",
    "numberOfClusters = pd.DataFrame()    \n",
    "X = df_temp.drop(columns = ['partnerCodeISO', 'flowType','component', 'cereal']).fillna(0)        \n",
    "\n",
    "for clusters in range(2,maxClusterNumbers + 1):\n",
    "    kmeans = KMeans(n_clusters = clusters, random_state = randomStateValue).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    score_s = silhouette_score(X, labels)\n",
    "    score_c = calinski_harabasz_score(X, labels)\n",
    "    score_d = davies_bouldin_score(X, labels)\n",
    "\n",
    "    numberOfClusters = numberOfClusters.append({'cereal': cereal,\n",
    "                                                'clusters': clusters,\n",
    "                                                'silhouetteScore': score_s,\n",
    "                                                'calinskiHarabaszScore': score_c,\n",
    "                                                'daviesBouldinScore': score_d,\n",
    "                                                'flowType':flowType\n",
    "                                                }, ignore_index = True) \n",
    "    \n",
    "numberOfClusters['maxValueOf_S'] = numberOfClusters['silhouetteScore'] == numberOfClusters['silhouetteScore'].max()\n",
    "numberOfClusters['maxValueOf_C'] = numberOfClusters['calinskiHarabaszScore'] == numberOfClusters['calinskiHarabaszScore'].max()\n",
    "numberOfClusters['minValueOf_D'] = numberOfClusters['daviesBouldinScore'] == numberOfClusters['daviesBouldinScore'].min()\n",
    "\n",
    "del(df_temp)     \n",
    "    \n",
    "optimalNumberOfClusters = numberOfClusters[(numberOfClusters['maxValueOf_S']) | (numberOfClusters['maxValueOf_C']) | (numberOfClusters['minValueOf_D'])].reset_index(drop = True)\n",
    "optimalNumberOfClusters = optimalNumberOfClusters.clusters.unique()\n",
    "optimalNumberOfClusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73265e-b47e-45cc-bcc4-9b5c5112c24b",
   "metadata": {},
   "source": [
    "Each metric suggests a different number of optimal clusters (limited to 6 possible clustersas the max number of clusters tested in the present project).\n",
    "\n",
    "|**METRIC**             | **#CLUSTERS**|\n",
    "|-----------------------|--------------|\n",
    "|Silohuette Index       | 2 Clusters |\n",
    "|Calinski Harabasz Score| 3 Clusters |\n",
    "|Davies-Bouldin Index   | 3 Clusters |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8325fe-644f-46ae-809d-d0e0b2b351d7",
   "metadata": {},
   "source": [
    "#### 02.02. CLUSTERING BY K-MEANS\n",
    "The K-means algorithm has been selected due to its interpretability; each cluster has a centroid with the mean value for each evaluated variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb21e6b-e480-4a9f-bcb4-710fd6b07f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- DATA (X)\n",
    "if target == 'trend':\n",
    "    df_temp = df_trend.copy()\n",
    "elif target == 'seasonal':\n",
    "    df_temp = df_seasonal.copy()\n",
    "elif target == 'residual':\n",
    "    df_temp = df_residual.copy()\n",
    "    \n",
    "X = df_temp.drop(columns = ['partnerCodeISO', 'flowType','component', 'cereal']).fillna(0)   \n",
    "\n",
    "##-- RESOURCES\n",
    "partnerCodes = df_base.partnerCodeISO.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1358d4b-1842-49eb-8aef-069d51c88f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##--CLUSTERING\n",
    "Y = X.copy().reset_index()\n",
    "newColumns = []\n",
    "kmeansCenters = pd.DataFrame()\n",
    "\n",
    "for numClusters in optimalNumberOfClusters:\n",
    "    clusterTag = f'{numClusters}_clusters{target.title()}'\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = numClusters, random_state = randomStateValue).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    centers = pd.DataFrame(kmeans.cluster_centers_, columns = X.columns)\n",
    "    centers['cluster'] = centers.index\n",
    "    centers['exp'] = clusterTag           \n",
    "    \n",
    "    Y[clusterTag] = labels\n",
    "    centers['partners'] = Y.groupby(clusterTag).size().values ##-PARTNERS BY CLUSTERS\n",
    "    \n",
    "    kmeansCenters = kmeansCenters.append(centers).reset_index(drop = True)\n",
    "    newColumns = np.append(newColumns,clusterTag)\n",
    "    \n",
    "    df_base = df_base.merge(Y[['partnerCodeISO',clusterTag]], on = 'partnerCodeISO', how = 'left').set_index('partnerCodeISO')\n",
    "    df_timeSeriesDecomp = df_timeSeriesDecomp.merge(Y[['partnerCodeISO',clusterTag]], on = 'partnerCodeISO', how = 'left').set_index('partnerCodeISO')\n",
    "                                  \n",
    "newColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d7e0b-efd7-4e62-b2d8-d7a625c12177",
   "metadata": {},
   "source": [
    "#### 02.03 SERIES COMPARISON\n",
    "\n",
    "Once the optimal number of clusters to test has been determined it's time to evaluate the results, in this case, we don't have multiple variables to plot in a radar chart for analyzing the characteristics of each cluster so, we can compare distirbutions and series to observe if the shapes and characteristcs of a cluster are similar.\n",
    "\n",
    "For this analysis, the series will be plotted by cluster standardized and with each cluster centroid for a better comprehension of the cluster pattern and behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a4dfd-dab5-4115-823b-e64c01a6af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterIndex = 0\n",
    "\n",
    "dfPartnersByCluster = df_base.reset_index()[['partnerCodeISO',newColumns[clusterIndex]]].drop_duplicates().groupby(newColumns[clusterIndex]).agg(partners = ('partnerCodeISO','count')).reset_index().rename(columns = {newColumns[clusterIndex]:'cluster'}).T\n",
    "dfPartnersByCluster.columns = dfPartnersByCluster[dfPartnersByCluster.index == 'cluster'].values[0]\n",
    "display(dfPartnersByCluster.iloc[1:,:])\n",
    "\n",
    "fig = grxClusters_stdDecomp(df = df_timeSeriesDecomp, target = target, flowType = flowType, clusterIndex = clusterIndex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797f21f-5226-4bab-bdfc-8073d72cf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterIndex = 1\n",
    "\n",
    "dfPartnersByCluster = df_base.reset_index()[['partnerCodeISO',newColumns[clusterIndex]]].drop_duplicates().groupby(newColumns[clusterIndex]).agg(partners = ('partnerCodeISO','count')).reset_index().rename(columns = {newColumns[clusterIndex]:'cluster'}).T\n",
    "dfPartnersByCluster.columns = dfPartnersByCluster[dfPartnersByCluster.index == 'cluster'].values[0]\n",
    "display(dfPartnersByCluster.iloc[1:,:])\n",
    "\n",
    "fig = grxClusters_stdDecomp(df = df_timeSeriesDecomp, target = target, flowType = flowType, clusterIndex = clusterIndex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f110fb-ea0c-462a-83f1-1ad1c9aea327",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 03.00. CLUSTERING BY SEASONAL COMPONENT\n",
    "#### 03.01. OPTIMAL NUMBER OF CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0ceb8-8b9e-471b-a285-e710523061c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- OPTIMAL CLUSTER NUMBERS\n",
    "target = 'seasonal'\n",
    "\n",
    "if target == 'trend':\n",
    "    df_temp = df_trend.copy()\n",
    "elif target == 'seasonal':\n",
    "    df_temp = df_seasonal.copy()\n",
    "elif target == 'residual':\n",
    "    df_temp = df_residual.copy()\n",
    "    \n",
    "numberOfClusters = pd.DataFrame()    \n",
    "X = df_temp.drop(columns = ['partnerCodeISO', 'flowType','component', 'cereal']).fillna(0)        \n",
    "\n",
    "for clusters in range(2,maxClusterNumbers + 1):\n",
    "    kmeans = KMeans(n_clusters = clusters, random_state = randomStateValue).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    score_s = silhouette_score(X, labels)\n",
    "    score_c = calinski_harabasz_score(X, labels)\n",
    "    score_d = davies_bouldin_score(X, labels)\n",
    "\n",
    "    numberOfClusters = numberOfClusters.append({'cereal': cereal,\n",
    "                                                'clusters': clusters,\n",
    "                                                'silhouetteScore': score_s,\n",
    "                                                'calinskiHarabaszScore': score_c,\n",
    "                                                'daviesBouldinScore': score_d,\n",
    "                                                'flowType':flowType\n",
    "                                                }, ignore_index = True) \n",
    "    \n",
    "numberOfClusters['maxValueOf_S'] = numberOfClusters['silhouetteScore'] == numberOfClusters['silhouetteScore'].max()\n",
    "numberOfClusters['maxValueOf_C'] = numberOfClusters['calinskiHarabaszScore'] == numberOfClusters['calinskiHarabaszScore'].max()\n",
    "numberOfClusters['minValueOf_D'] = numberOfClusters['daviesBouldinScore'] == numberOfClusters['daviesBouldinScore'].min()\n",
    "\n",
    "del(df_temp)     \n",
    "    \n",
    "optimalNumberOfClusters = numberOfClusters[(numberOfClusters['maxValueOf_S']) | (numberOfClusters['maxValueOf_C']) | (numberOfClusters['minValueOf_D'])].reset_index(drop = True)\n",
    "optimalNumberOfClusters = optimalNumberOfClusters.clusters.unique()\n",
    "optimalNumberOfClusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1cb64-b8ad-4d26-a7b7-033e80853d90",
   "metadata": {},
   "source": [
    "#### 03.02. CLUSTERING BY K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ffaf8-c418-461f-97d0-c328a32bb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- DATA (X)\n",
    "if target == 'trend':\n",
    "    df_temp = df_trend.copy()\n",
    "elif target == 'seasonal':\n",
    "    df_temp = df_seasonal.copy()\n",
    "elif target == 'residual':\n",
    "    df_temp = df_residual.copy()\n",
    "    \n",
    "X = df_temp.drop(columns = ['partnerCodeISO', 'flowType','component', 'cereal']).fillna(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79576a57-5b3b-4247-a01f-0ebe3dc86b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "##--CLUSTERING\n",
    "Y = X.copy().reset_index()\n",
    "newColumns = []\n",
    "#kmeansCenters = pd.DataFrame()\n",
    "\n",
    "for numClusters in optimalNumberOfClusters:\n",
    "    clusterTag = f'{numClusters}_clusters{target.title()}'\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = numClusters, random_state = randomStateValue).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    centers = pd.DataFrame(kmeans.cluster_centers_, columns = X.columns)\n",
    "    centers['cluster'] = centers.index\n",
    "    centers['exp'] = clusterTag           \n",
    "    \n",
    "    Y[clusterTag] = labels\n",
    "    centers['partners'] = Y.groupby(clusterTag).size().values ##-PARTNERS BY CLUSTERS\n",
    "    \n",
    "    kmeansCenters = kmeansCenters.append(centers).reset_index(drop = True)\n",
    "    newColumns = np.append(newColumns,clusterTag)\n",
    "    \n",
    "    df_base = df_base.merge(Y[['partnerCodeISO',clusterTag]], on = 'partnerCodeISO', how = 'left').set_index('partnerCodeISO')\n",
    "    df_timeSeriesDecomp = df_timeSeriesDecomp.merge(Y[['partnerCodeISO',clusterTag]], on = 'partnerCodeISO', how = 'left').set_index('partnerCodeISO')\n",
    "                                  \n",
    "newColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9618af-2baf-4f13-b267-0b91d187ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterIndex = 0\n",
    "\n",
    "dfPartnersByCluster = df_base.reset_index()[['partnerCodeISO',newColumns[clusterIndex]]].drop_duplicates().groupby(newColumns[clusterIndex]).agg(partners = ('partnerCodeISO','count')).reset_index().rename(columns = {newColumns[clusterIndex]:'cluster'}).T\n",
    "dfPartnersByCluster.columns = dfPartnersByCluster[dfPartnersByCluster.index == 'cluster'].values[0]\n",
    "display(dfPartnersByCluster.iloc[1:,:])\n",
    "\n",
    "fig = grxClusters_stdDecomp(df = df_timeSeriesDecomp, target = target, flowType = flowType, clusterIndex = clusterIndex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7220db8-54cd-4033-af89-5718bd8f8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterIndex = 1\n",
    "\n",
    "dfPartnersByCluster = df_base.reset_index()[['partnerCodeISO',newColumns[clusterIndex]]].drop_duplicates().groupby(newColumns[clusterIndex]).agg(partners = ('partnerCodeISO','count')).reset_index().rename(columns = {newColumns[clusterIndex]:'cluster'}).T\n",
    "dfPartnersByCluster.columns = dfPartnersByCluster[dfPartnersByCluster.index == 'cluster'].values[0]\n",
    "display(dfPartnersByCluster.iloc[1:,:])\n",
    "\n",
    "fig = grxClusters_stdDecomp(df = df_timeSeriesDecomp, target = target, flowType = flowType, clusterIndex = clusterIndex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe635c-87cb-4fe9-9c00-a5ef68267869",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 04.00. CLUSTERING BY RESIDUAL COMPONENT\n",
    "#### 04.01. OPTIMAL NUMBER OF CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0efff-640d-453d-ab2e-9e3ba229c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- OPTIMAL CLUSTER NUMBERS\n",
    "target = 'residual'\n",
    "\n",
    "if target == 'trend':\n",
    "    df_temp = df_trend.copy()\n",
    "elif target == 'seasonal':\n",
    "    df_temp = df_seasonal.copy()\n",
    "elif target == 'residual':\n",
    "    df_temp = df_residual.copy()\n",
    "    \n",
    "numberOfClusters = pd.DataFrame()    \n",
    "X = df_temp.drop(columns = ['partnerCodeISO', 'flowType','component', 'cereal']).fillna(0)        \n",
    "\n",
    "for clusters in range(2,maxClusterNumbers + 1):\n",
    "    kmeans = KMeans(n_clusters = clusters, random_state = randomStateValue).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    score_s = silhouette_score(X, labels)\n",
    "    score_c = calinski_harabasz_score(X, labels)\n",
    "    score_d = davies_bouldin_score(X, labels)\n",
    "\n",
    "    numberOfClusters = numberOfClusters.append({'cereal': cereal,\n",
    "                                                'clusters': clusters,\n",
    "                                                'silhouetteScore': score_s,\n",
    "                                                'calinskiHarabaszScore': score_c,\n",
    "                                                'daviesBouldinScore': score_d,\n",
    "                                                'flowType':flowType\n",
    "                                                }, ignore_index = True) \n",
    "    \n",
    "numberOfClusters['maxValueOf_S'] = numberOfClusters['silhouetteScore'] == numberOfClusters['silhouetteScore'].max()\n",
    "numberOfClusters['maxValueOf_C'] = numberOfClusters['calinskiHarabaszScore'] == numberOfClusters['calinskiHarabaszScore'].max()\n",
    "numberOfClusters['minValueOf_D'] = numberOfClusters['daviesBouldinScore'] == numberOfClusters['daviesBouldinScore'].min()\n",
    "\n",
    "del(df_temp)     \n",
    "    \n",
    "optimalNumberOfClusters = numberOfClusters[(numberOfClusters['maxValueOf_S']) | (numberOfClusters['maxValueOf_C']) | (numberOfClusters['minValueOf_D'])].reset_index(drop = True)\n",
    "optimalNumberOfClusters = optimalNumberOfClusters.clusters.unique()\n",
    "optimalNumberOfClusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed44336-44ca-4243-8bd5-7437fb21d2ca",
   "metadata": {},
   "source": [
    "#### 04.02. CLUSTERING BY K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2fa923-1f55-40e3-b074-8467201ca370",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- DATA (X)\n",
    "if target == 'trend':\n",
    "    df_temp = df_trend.copy()\n",
    "elif target == 'seasonal':\n",
    "    df_temp = df_seasonal.copy()\n",
    "elif target == 'residual':\n",
    "    df_temp = df_residual.copy()\n",
    "    \n",
    "X = df_temp.drop(columns = ['partnerCodeISO', 'flowType','component', 'cereal']).fillna(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b62cc-3ff7-42e4-950e-09f5b29b9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##--CLUSTERING\n",
    "Y = X.copy().reset_index()\n",
    "newColumns = []\n",
    "#kmeansCenters = pd.DataFrame()\n",
    "\n",
    "for numClusters in optimalNumberOfClusters:\n",
    "    clusterTag = f'{numClusters}_clusters{target.title()}'\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = numClusters, random_state = randomStateValue).fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    centers = pd.DataFrame(kmeans.cluster_centers_, columns = X.columns)\n",
    "    centers['cluster'] = centers.index\n",
    "    centers['exp'] = clusterTag           \n",
    "    \n",
    "    Y[clusterTag] = labels\n",
    "    centers['partners'] = Y.groupby(clusterTag).size().values ##-PARTNERS BY CLUSTERS\n",
    "    \n",
    "    kmeansCenters = kmeansCenters.append(centers).reset_index(drop = True)\n",
    "    newColumns = np.append(newColumns,clusterTag)\n",
    "    \n",
    "    df_base = df_base.merge(Y[['partnerCodeISO',clusterTag]], on = 'partnerCodeISO', how = 'left').set_index('partnerCodeISO')\n",
    "    df_timeSeriesDecomp = df_timeSeriesDecomp.merge(Y[['partnerCodeISO',clusterTag]], on = 'partnerCodeISO', how = 'left').set_index('partnerCodeISO')\n",
    "                                  \n",
    "newColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dc1fe-0864-48bd-b137-00bd53f8c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterIndex = 0\n",
    "\n",
    "dfPartnersByCluster = df_base.reset_index()[['partnerCodeISO',newColumns[clusterIndex]]].drop_duplicates().groupby(newColumns[clusterIndex]).agg(partners = ('partnerCodeISO','count')).reset_index().rename(columns = {newColumns[clusterIndex]:'cluster'}).T\n",
    "dfPartnersByCluster.columns = dfPartnersByCluster[dfPartnersByCluster.index == 'cluster'].values[0]\n",
    "display(dfPartnersByCluster.iloc[1:,:])\n",
    "\n",
    "fig = grxClusters_stdDecomp(df = df_timeSeriesDecomp, target = target, flowType = flowType, clusterIndex = clusterIndex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f689bf-7567-4faf-b1c4-fde160c50c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterIndex = 1\n",
    "\n",
    "dfPartnersByCluster = df_base.reset_index()[['partnerCodeISO',newColumns[clusterIndex]]].drop_duplicates().groupby(newColumns[clusterIndex]).agg(partners = ('partnerCodeISO','count')).reset_index().rename(columns = {newColumns[clusterIndex]:'cluster'}).T\n",
    "dfPartnersByCluster.columns = dfPartnersByCluster[dfPartnersByCluster.index == 'cluster'].values[0]\n",
    "display(dfPartnersByCluster.iloc[1:,:])\n",
    "\n",
    "fig = grxClusters_stdDecomp(df = df_timeSeriesDecomp, target = target, flowType = flowType, clusterIndex = clusterIndex)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1bedc-52e5-4160-827c-45857657885b",
   "metadata": {},
   "source": [
    "### 05.00. EXPORT\n",
    "\n",
    "The results will be exported to be presented in an exploratory dashboard with the other clustering experiments results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38878d-21c2-4bc2-9865-6d93386b3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedExperiment = ['3_clustersTrend','3_clustersSeasonal','3_clustersResidual']\n",
    "\n",
    "df_export = df_base[selectedExperiment].reset_index().drop_duplicates().reset_index(drop = True).rename(columns = {selectedExperiment[0]:f'clusterBy{selectedExperiment[0][10:]}Component',\n",
    "                                                                                                                   selectedExperiment[1]:f'clusterBy{selectedExperiment[1][10:]}Component',\n",
    "                                                                                                                   selectedExperiment[2]:f'clusterBy{selectedExperiment[2][10:]}Component'})\n",
    "df_export['cereal'] = cereal\n",
    "df_export.to_csv(wd + f'/clustersExp03.csv.gz', index = False, sep = csvAttr_imp['sep'], encoding = csvAttr_imp['encoding'])\n",
    "df_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09e818-76da-4e3b-810e-4cd7f0609c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export_centers = kmeansCenters[kmeansCenters['exp'].isin(selectedExperiment)].drop(columns = ['partners']).melt(id_vars = ['cluster','exp']).rename(columns = {'value':f'{flowColumn}_scaledCentroid'})\n",
    "df_export_centers['cereal'] = cereal\n",
    "df_export_centers.to_csv(wd + f'/clustersExp03_centers.csv.gz', index = False, sep = csvAttr_imp['sep'], encoding = csvAttr_imp['encoding'])\n",
    "df_export_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d71db-b743-4e8b-b667-d201d2b7d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting Base dataset\n",
    "df_auxClusterA = pd.read_csv(wd + '/clustersExp01.csv.gz', sep = csvAttr_imp['sep'], encoding = csvAttr_imp['encoding'])\n",
    "df_auxClusterB = pd.read_csv(wd + '/clustersExp02.csv.gz', sep = csvAttr_imp['sep'], encoding = csvAttr_imp['encoding'])\n",
    "\n",
    "df_export_base = df_base_copy.merge(df_auxClusterA[['partnerCodeISO','clusterByMetrics']], on = 'partnerCodeISO', how = 'left')\n",
    "df_export_base = df_export_base.merge(df_auxClusterB[['partnerCodeISO','clusterByTWDs']], on = 'partnerCodeISO', how = 'left')\n",
    "df_export_base = df_export_base.merge(df_export[['partnerCodeISO','clusterByTrendComponent','clusterBySeasonalComponent','clusterByResidualComponent']], on = 'partnerCodeISO', how = 'left')\n",
    "\n",
    "df_export_base.to_csv(wd + f'/df_baseWithClusters.csv.gz', index = False, sep = csvAttr_imp['sep'], encoding = csvAttr_imp['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23110a3-e884-42c9-87f3-ee741809ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export_base.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
